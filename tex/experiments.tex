\section{Experimental results}\label{experiments}

Figure \ref{fig:experimental_flow} presents our methodology to obtain accuracy and power profiles for several degrees of approximation, and to generate a version with configurable approximations. 
\par We begin by implementing EKF in Matlab and applying several algorithm-specific approximations (described in detail in Sub-Section \ref{subsec:profiles}). Matlab simulations are performed to obtain accuracy profiles for each approximation at this step. Matlab Coder is then used to generate C code corresponding to the exact and various approximated versions. C code generated by Matlab is not directly synthesizable to FPGA: hence, we refactor it manually to obtain synthesizable versions. Functionality and corresponding accuracy are not affected by this step. We then apply algorithm-independent approximations on this refactored C code, and use a High Level Synthesis tool (Xilinx Vivado HLS) to generate Verilog Hardware Description Language (Verilog HDL) exact and approximated versions. Matlab's HDL coder offers a direct route from Matlab to FPGA, but not all language constructs are supported and it would not enable us to perform algorithm-independent optimisations: hence the use of HLS through Xilinx's tools.
\par RTL simulation and FPGA synthesis (we target Virtex 7 technology) is performed using Xilinx Vivado Design Suite to obtain resource usage and power consumption. Power consumption is estimated through Xilinx Power Estimator tool embedded in Vivado, which uses resource usage information and switching rates obtained from RTL simulation to calculate a high accuracy measure of power consumption. This step enables us to obtain power profiles for each approximation.
\par The final step combines the various versions into one configurable solution. This is an FPGA implementation which combines accurate and approximated versions, where the level of approximations can be configured and modified at runtime. Unused logic (e.g., an exact version of some operation when running in corresponding approximate mode) is clock gated to eliminate dynamic power consumption. This version is then used to obtain results for dynamic approximations by the approximation engine using prior knowledge.  

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.9\columnwidth]{img/experimental_flow.png}
  \caption{Experimental design flow.}
  \label{fig:experimental_flow}
\end{figure}

\subsection{Power and Accuracy Profiles}\label{subsec:profiles}

Algorithm-dependent approximations are modifications on the implementation of certain constructs, which vary from algorithm to algorithm. These can be complete re-writes of algorithm functionality; however, since our goal is to determine the validity of prior knowledge for runtime approximations, simple arithmetic re-writes suffice. We implemented four different re-writes:

\begin{enumerate}
\item COS re-write: in the exact HDL version, cosine functions are implemented using look-up tables with 180 entries. In the approximated version, these are quantised to use only 90 entries, requiring fewer data than the exact version.
\item SIN re-write: identically to cosine, sine functions are re-written using a quantized loop-up table.
\item SQRT re-write: the square root of the sum of squares $\sqrt{x^2+y^2}$, which requires two DSP multipliers and a look-up table for the square root, is replaced by a sum of absolutes $\left| x\right|+\left| y\right|$, which requires only two conversions to unsigned and an adder.
\item ATAN re-write: the arc tangent function is realized in the exact version through a look up table. In the approximate version, it is approximated through the first three terms of a Taylor series, requiring only arithmetic operations, thus reducing memory requirements.
\end{enumerate}

\par We detailed several algorithm-independent approximations in Section \ref{learning}. In our experiments, we implemented bit width reduction, from floating to fixed point. The default bit width is 64 bits (Matlab generates double precision floating point operations). Our approximations replaced 64 bits data with fixed point 27 bits data (15 integer and 12 fractional bits). This is one of the fixed point bit widths recommended for power reductions in the Virtex 7 family. 

\begin{table}[h]
\begin{tabular}{r c c c c}
\toprule
Version & \multicolumn{3}{c}{Power (W)} & \% of baseline\\
 & Static & Dynamic & Total &\\
\hline
Exact & 0.247 & 0.552 & 0.799 & 100\%\\
COS & 0.247 & 0.523 & 0.770 & 96.37\%\\
SIN & 0.247 & 0.523 & 0.770 & 96.37\%\\
SQRT & 0.247 & 0.534 & 0.781 & 97.74\%\\
ATAN & 0.247 & 0.532 & 0.779 & 97.49\%\\
27 bits Fixed Point & 0.246 & 0.538 & 0.785 & 98.24\%\\
\hline
\end{tabular}
\caption{Power profiles for exact and each individual approximation.}
\label{table:power_profiles}
\end{table}


\par We ensured that during our RTL simulations, 100 iterations of the complete function are performed, using the same randomly generated input data for all versions, in order to obtain representative activity that ensures high confidence in the power estimation. Table \ref{table:power_profiles} depicts power consumption for the exact and each individual approximated version. Static power remains constant across approximations because the contribution of on-chip memories (BRAMs) dominates, and the number remains constant except for a small reduction when reducing bit widths. 
\par Our next set of experiments started measuring the power consumption of versions that combine bit width reduction with several of the other approximations: results are presented in Table \ref{table:power_profiles_comb}.



\begin{table}[h]
\begin{tabular}{r c c}
\toprule
Approximations (using 27FP) & Power (W) & \% of baseline\\
\hline
COS & 						0.756	& 94.61\%\\
SIN & 						0.756	& 94.61\%\\
SQRT & 						0.767	& 95.99\%\\
ATAN & 						0.765	& 95.74\%\\
COS \& SIN & 				0.741	& 92.74\%\\
COS \& SQRT& 				0.767	& 95.99\%\\
COS \& ATAN & 				0.736	& 92.11\%\\
SIN \& SQRT & 				0.738	& 92.36\%\\
SIN \& ATAN & 				0.736	& 92.11\%\\
SQRT \& ATAN & 				0.747	& 93.49\%\\
COS \& SIN \& SQRT & 		0.709	& 88.73\%\\
COS \& SIN \& ATAN& 		0.707	& 88.48\%\\
COS \& SQRT \& ATAN& 		0.718	& 89.86\%\\
SIN \& SQRT \& ATAN  & 		0.718	& 89.86\%\\
COS \& SIN \& SQRT \& ATAN &0.689 	& 86.23\%\\
\hline
\end{tabular}
\caption{Power profiles for combinations of different approximations.}
\label{table:power_profiles_comb}
\end{table}


\par We use Matlab simulation to determine the accuracy of each combination, compared to the baseline. Table \ref{table:acc_profiles} depicts the accuracy of each version. Results in the "Absolute" tab are the mean and standard deviation ($\sigma$), respectively, of the absolute error (Euclidean distance in meters) compared to the ground truth (real object position) in our simulation. Results in the "Accuracy" tab are the ratios between the exact version results and the absolute value of the difference between exact and approximate results.
    


\begin{table}[h]
\begin{tabular}{r c c c}
\toprule
Version & \multicolumn{2}{c}{Absolute (m)} & Accuracy (\%)\\
 & Mean & $\sigma$ & \\
\hline
Exact	& 0.5633 & 0.3092 & 100\\
\hline
COS & 						0.6742	& 0.2805	& 83.55 \\
SIN & 						0.6524	& 0.3652	& 86.34 \\
SQRT & 						0.6968	& 0.3764	& 80.84 \\
ATAN & 						3.3520	& 1.7953	& 16.80 \\
COS \& SIN & 				0.6044	& 0.3973	& 93.19 \\
COS \& SQRT& 				0.7497	& 0.4101	& 75.13 \\
COS \& ATAN & 				0.9771	& 0.6937	& 57.65 \\
SIN \& SQRT & 				0.7353	& 0.3711	& 76.60 \\
SIN \& ATAN & 				7.7667	& 2.5248	& 7.252 \\
SQRT \& ATAN & 				1.4812	& 0.8149	& 38.02 \\
COS \& SIN \& SQRT & 		0.5870	& 0.3680	& 95.96 \\
COS \& SIN \& ATAN& 		1.7503	& 1.0135	& 32.18 \\
COS \& SQRT \& ATAN& 		0.9949	& 0.7006	& 56.61 \\
SIN \& SQRT \& ATAN  & 		7.7528	& 2.5424	& 7.265 \\
COS \& SIN \& SQRT \& ATAN &1.8295 	& 1.0765	& 30.78 \\
\hline
\end{tabular}
\caption{Accuracy profiles for combinations of different approximations, for 100 iterations. Exact version uses double floating point precision (64 bits); every other version uses 27 bits fixed point precision.}
\label{table:acc_profiles}
\end{table}

\par The final integrated version with configurable approximations, including clock gated logic, is compared against the baseline in Table \ref{table:integrated}.

%\begin{table}[h]
%\begin{tabular}{r c c c c c c c}
%\toprule
% & \multicolumn{3}{c}{Power (W)} & \multicolumn{4}{c}{FPGA Resources}\\
% & Static & Dynamic & Total & LUT & FF & DSP & BRAM\\
%\hline
%Exact & 0.247 & 0.552 & 0.799 & 46449 & 17666 & 61 & 55\\
%Approx. & 0.246 & 0.443 & 0.689 & 44995 & 16981 & 58 & 55\\
%\hline
%\end{tabular}
%\caption{Resource usage and ower consumption for baseline and final configurable approximations version.}
%\label{table:integrated}
%\end{table}

\begin{table}[h]
\begin{tabular}{r c c c c c}
\toprule
 & Power (W) & \multicolumn{4}{c}{FPGA Resources}\\
 & Total & LUT & FF & DSP & BRAM\\
\hline
Exact & 0.799 & 46449 & 17666 & 61 & 55\\
Approx. & 0.689 & 44995 & 16981 & 58 & 55\\
\hline
\end{tabular}
\caption{Resource usage and ower consumption for baseline and final configurable approximations version.}
\label{table:integrated}
\end{table}



\subsection{Dynamic approximations using prior knowledge}
The results of dynamically performing the approximation using the algorithms explained in Section \ref{learning} and \ref{sec:EKF} are detailed here. First, using the approximation results of each individual step, an "approximation level" is defined. To do this, various combinations of the micro approximations are sorted in ascending order according to their accuracy error. This results in a vector of tuples containing pairs of indexes (used as the approximation level) and strings indicating the approximation combination. 

The first tuple in this vector corresponds to the approximation method, which generates the highest accuracy. Its last element, however, creates the weakest accuracy.
At each filtering step, in order to use different approximation combinations, the approximation level is either increased or decreased. Determining whether to increment or decrement the approximation level is performed by evaluating the KL divergence result (\ref{eq:KLDIV2}).

%\begin{figure}[tb]
%  \centering
%  \includegraphics[width=0.9\columnwidth]{img/Tracks_plot_cropped.eps}
%  \caption{Every 100 iteration overlay plot of the tracks and true states for an overall 5000 iterations.}
%  \label{fig:track_res}
%\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[tb]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/KL_0_5.png}
        \caption{}
        %\label{fig:gull}
    \end{subfigure}
    \begin{subfigure}[tb]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/KL_1.png}
        \caption{}
        %\label{fig:tiger}
    \end{subfigure}
        \begin{subfigure}[tb]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/KL_2.png}
        \caption{}
        %\label{fig:gull}
    \end{subfigure}
        \begin{subfigure}[tb]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/KL_3.png}
        \caption{}
        %\label{fig:gull}
    \end{subfigure}
    \caption{Tracking results, where red, green and blue show the true state, tracking without approximation and tracking after approximation, respectively, using KL divergence threshold: (a) 0.5, (b) 1, (c) 2, (d) 3.}
    \label{fig:track_res}
\end{figure}







%\begin{figure}
%    \centering
%    \begin{subfigure}[tb]{0.2\textwidth}
%        \includegraphics[width=\textwidth]{img/Hist_KL_0_5.png}
%        \caption{}
%        %\label{fig:gull}
%    \end{subfigure}
%    \begin{subfigure}[tb]{0.2\textwidth}
%        \includegraphics[width=\textwidth]{img/Hist_KL_1.png}
%        \caption{}
%        %\label{fig:tiger}
%    \end{subfigure}
%        \begin{subfigure}[tb]{0.2\textwidth}
%        \includegraphics[width=\textwidth]{img/Hist_KL_2.png}
%        \caption{}
        %\label{fig:gull}
%    \end{subfigure}
%        \begin{subfigure}[tb]{0.2\textwidth}
%        \includegraphics[width=\textwidth]{img/Hist_KL_3.png}
%        \caption{}
        %\label{fig:gull}
 %   \end{subfigure}
 %   \caption{Histogram of approximation levels using KL divergence threshold in Fig.~\ref{fig:track_res}: (a) 0.5, (b) 1, (c) 2, (d) 3.}
 %   \label{fig:track_hist}
%\end{figure}














The result of the overlay tracking plot using dynamic approximation in various iterations is shown in Fig.~\ref{fig:track_res}. 
%The red ellipses represent the covariance matrix ${\bf \Sigma}_{X,k}$ (\ref{eq:correctCOV}) at every iteration, the blue circles show $[x_k, y_k]^\intercal$ and green asterisks with red dots are the locations of true states $[\bar{x}_k, \bar{y}_k]^\intercal$. 
Red shows ground truth, green shows tracking without approximation and blue shows tracking after approximation, using KL divergence thresholds of 0.5, 1, 2, 3.

At every iteration, KL divergence is computed according to (\ref{eq:KLDIV2}), with sample stack size $N = 75$.  If it is less than a given threshold $T$ (for our results in Fig. \ref{fig:track_res}, $T = 0.5$, 1, 2 and 3) the approximation level is incremented; otherwise, it is decremented to have a more precision tracking performance. %Fig. \ref{fig:track_hist} displays histograms of approximation levels, corresponding to the results in Fig. \ref{fig:track_res}. 
The Euclidean distance between the found track and true state is computed for each iteration and shown in Fig.~\ref{fig:KL_and_err}-a, as well as the KL divergence in Fig.~\ref{fig:KL_and_err}-b. When the KL divergence goes above $T$ in Fig.~\ref{fig:KL_and_err}, the accuracy deteriorates. The approximation level is then immediately reduced in order to compensate the accuracy. Therefore, in only a few more iterations the accuracy again increases. This causes median accuracy error of 0.1661 (m) with median KL divergence 1.2098 for 5000 iterations.

\begin{figure}
    \centering
    \begin{subfigure}[tb]{0.465\textwidth}
        \includegraphics[width=\textwidth]{img/Err_figure_cropped.eps}
        \caption{}
        \label{fig:gull}
    \end{subfigure}
    \begin{subfigure}[tb]{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/KL_figure_cropped.eps}
        \caption{}
        \label{fig:tiger}
    \end{subfigure}
    \caption{(a) Euclidean distance between the true state and tracks; (b) Corresponding KL divergence.}
    \label{fig:KL_and_err}
\end{figure}

Using the power profiles in Table \ref{table:power_profiles_comb}, the consumed energy of each approximation can be computed by multiplying the power with the time resolution ($\Delta t = 0.1$ (s)). The accumulated energy is plotted in Fig. \ref{fig:Energy_res_acc} for the last 1500 iterations. 
The baseline which consumes a constant power of 0.799 (W), results in a total of $\approx 394$ (J). However, the accumulated energy consumption for the dynamic approximation case is $\approx 384$ (J). This shows about $2.54 \%$ energy reduction.
By compromising the accuracy and increasing the KL divergence threshold, more intense approximations can be obtained, which results in even lower energy consumption. This is shown in Fig.~\ref{fig:Energy_res_acc} as the dashed green and cyan lines for thresholds 1.5 and 4.0, respectively

\begin{figure}[tb]
  \centering
  \includegraphics[width=1\columnwidth]{img/Energy_cropped.eps}
  \caption{Accumulated energy consumption for the baseline (no approximation) and dynamic approximations, with three different KL divergence thresholds.}
  \label{fig:Energy_res_acc}
\end{figure}
